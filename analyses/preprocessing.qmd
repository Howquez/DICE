---
title: 'Unveiling Social Media Realities'
subtitle: 'Advanced Causal Inference through In-Context Testing'
author:
- name: Hauke Roggenkamp
  email: Hauke.Roggenkamp@unisg.ch
  orcid: 0009-0005-5176-4718
  corresponding: true
  affiliations:
    - name: Institute of Behavioral Science and Technology, University of St. Gallen
      address: Torstrasse 25
      city: St. Gallen
      country: Switzerland
      postal-code: 9000
    - name: Faculty of Economics & Social Sciences, Helmut-Schmidt-University
      address: Holstenhofweg 85
      city: Hamburg
      country: Germany
      postal_code: 22043
- name: Johannes Boegershausen
  email: boegershausen@rsm.nl
  orcid: 0000-0002-1429-9344
  corresponding: false
  affiliations:
    - name: Rotterdam School of Management, Erasmus University 
      address: Burgemeester Oudlaan 50
      city: Rotterdam
      country: Netherlands
      postal-code: 3062
- name: Christian Hildebrand
  email: Christian.Hildebrand@unisg.ch
  orcid: 0000-0003-4366-3093
  corresponding: false
  affiliations:
    - name: Institute of Behavioral Science and Technology, University of St. Gallen
      address: Torstrasse 25
      city: St. Gallen
      country: Switzerland
      postal-code: 9000
date: now
date-format: long
format: 
  html:
    embed-resources: true
    theme: cosmo
toc: false
number-sections: false
fig-cap-location: top
---

## Install Packages

```{r install_packages}

if (!requireNamespace("groundhog", quietly = TRUE)) {
    install.packages("groundhog")
    library("groundhog")
}

pkgs <- c("magrittr", "data.table", "knitr", "stringr", "jsonlite", 
          "ggplot2", "patchwork")

groundhog::groundhog.library(pkg = pkgs,
                             date = "2024-01-01")

rm(pkgs)
```


## Read Data

```{r read_data}
raw <- data.table::fread(file = "../data/testing/all_apps_wide-2024-01-05 (2).csv")
```

## Select Data

```{r select_data}
cols <- str_detect(string = names(raw), 
                   pattern = "participant.label|participant.code|likes_data|replies_data|favorable|sequence|viewport|condition")

data <- raw[participant._index_in_pages >= 4, ..cols]

rm(cols)
```

## Rename Data

```{r rename_data}
names(data) %>%
  str_replace_all(pattern = ".*player\\.", replacement = "") %>%
  str_replace_all(pattern = "\\.", replacement = "_") %>%
  str_replace_all(pattern = "feed_", replacement = "") %>%
  str_to_lower() %>%
  setnames(x = data)
```

## Item Sequence

```{r}

display <- data[, 
             .(tweet = unlist(base::strsplit(x = sequence, 
                                             split = ", ")) %>% 
                 as.integer()),
             by = participant_code]

display[, displayed_sequence := 1:.N, by = participant_code]
```


## Scroll Sequence

Because people may scroll back and forth, this sequence may be longer than the number of tweets, simply because some tweets may occur several times in that scroll sequence.

```{r}

sequence <- data[nchar(viewport_data) > 1,
                 viewport_data %>%
                     str_replace_all(pattern = '""', 
                                     replacement = '"') %>%
                     fromJSON,
                 by = participant_code][!is.na(doc_id)][, scroll_sequence := 1:.N, by = participant_code]

setnames(sequence, old = 'doc_id', new = 'tweet')

flow <- sequence[display, on = .(participant_code, tweet)]

flow[, duplicate := duplicated(tweet), by = participant_code]

setorder(flow, participant_code, scroll_sequence)

rm(list = c('sequence'))
```

## Viewport

```{r transform_json_output}

viewport <- data[nchar(viewport_data) > 1,
                 fromJSON(str_replace_all(string = viewport_data,
                                 pattern = '""',
                                 replacement = '"')),
                 by = participant_code][!is.na(doc_id)]

# sum of durations by tweet (in case someone scrolled back and forth)
viewport <- viewport[, 
                     .(seconds_in_viewport = sum(duration, 
                                                 na.rm = TRUE)),
                     by = c('participant_code', 'doc_id')]


# rename
setnames(x = viewport,
         old = 'doc_id',
         new = 'tweet')
```


## Reactions

```{r}
likes <- data[nchar(likes_data) > 1,
                 fromJSON(str_replace_all(string = likes_data,
                                 pattern = '""',
                                 replacement = '"')),
                 by = participant_code][!is.na(doc_id)]

replies <- data[nchar(replies_data) > 1,
                 fromJSON(str_replace_all(string = replies_data,
                                 pattern = '""',
                                 replacement = '"')),
                 by = participant_code][!is.na(doc_id)]

reactions <- merge(likes, replies, by = c("participant_code", "doc_id"), all = TRUE)

# make sure doc_id is numeric as is the case for the other data.tables
reactions[, doc_id := as.numeric(doc_id)]

# rename
setnames(x = reactions,
         old = 'doc_id',
         new = 'tweet')

rm(list = c("replies", "likes"))
```


## Merge (stepwise)

```{r}
merge_1 <- merge(data[, .(participant_code, condition)], display, by = c("participant_code"), all = TRUE)
merge_2 <- merge(merge_1, viewport, by = c("participant_code", "tweet"), all = TRUE)
final <- merge(merge_2, reactions, by = c("participant_code", "tweet"), all = TRUE)

rm(list = c("merge_1", "merge_2"))
```


## Write Data

```{r}
fwrite(x = final,
       file = '../data/processed/tweet_level_data.csv')
```

