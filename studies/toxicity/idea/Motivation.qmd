---
title: 'DICE Demonstration'
subtitle: "Costs and Benefits of Toxicity Moderation"
author:
- name: Hauke Roggenkamp
  email: Hauke.Roggenkamp@unisg.ch
  orcid: 0009-0005-5176-4718
  corresponding: true
  affiliations:
    - name: Institute of Behavioral Science and Technology, University of St. Gallen
      address: Torstrasse 25
      city: St. Gallen
      country: Switzerland
      postal-code: 9000
date: now
date-format: long
format: 
  html:
    embed-resources: true
    theme: cosmo
    code-line-numbers: true
toc: false
number-sections: false
fig-cap-location: top
execute:
  echo: false
bibliography: ../../../documentation/references.bib
---

Meta estimates that its users [share a billion stories](https://web.archive.org/web/20240531061745/https://www.facebook.com/business/news/stories-can-do-it) (disapearing posts) every day. With every post, social media platforms such as Facebook, TikTok or X have the potential to become more attractive for users and advertisers: If the platform is designed well, each additional post increases the chances tha a user consumes and interacts with content that is relevant to her. This is likely to increase her time spent on the platform [@AridorEtAl_2024]. Advertisers are interested in the users' time and pay platforms to get a share of it.

In addition to facilitating the content production and consumption, the core of a platform's business is to distribute the content to its consumers. Due to the unprecedented scale of content production, social media platforms cannot expose all of its users to all the available content. Instead, they curate personalized subsets content. In contrast to traditional media outlets, this curation is specific to the individual user and algorithmic: instead of editors picking the most relevant content for all users, recommender systems create individual lists of content that are to be displayed.

Despite selecting potentially relevant content per user, platforms also have to filter content that shouldn't be displayed to _any_ user. As they cannot control the content production directly, platforms typically set rules that define which type of content shall (not) be displayed. However, actively moderating the content production by enforcing these rules and sanctioning violations is costly because platforms need to design algorithms that flag potentially unwanted content and employ human moderators who evaluate the flagged posts. These direct costs are -- perhaps counter-intuitively -- topped by indirect costs: Field experimental evidence suggests that the removal of harmful content^[This includes hate speech and misinformation, for instance and is considered harmful as the literature works with the assumption that such content imposes negative externalities on groups of the pupulation [@BeknazarYuzbashevEtAl_2022, p. 8]] reduces content consumption.

 

[Introduce (in-)direct costs of content moderation to then argue for benefits.]

# References


