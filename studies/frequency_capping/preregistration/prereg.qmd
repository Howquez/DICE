---
title: 'DICE Pre-Test'
subtitle: "How Much Is Enough? Exploring Frequency Capping in Social Media Advertising"
author:
- name: Hauke Roggenkamp
  email: Hauke.Roggenkamp@unisg.ch
  orcid: 0009-0005-5176-4718
  corresponding: true
  affiliations:
    - name: Institute of Behavioral Science and Technology, University of St. Gallen
      address: Torstrasse 25
      city: St. Gallen
      country: Switzerland
      postal-code: 9000
- name: Johannes Boegershausen
  email: boegershausen@rsm.nl
  orcid: 0000-0002-1429-9344
  corresponding: false
  affiliations:
    - name: Rotterdam School of Management, Erasmus University 
      address: Burgemeester Oudlaan 50
      city: Rotterdam
      country: Netherlands
      postal-code: 3062
- name: Christian Hildebrand
  email: Christian.Hildebrand@unisg.ch
  orcid: 0000-0003-4366-3093
  corresponding: false
  affiliations:
    - name: Institute of Behavioral Science and Technology, University of St. Gallen
      address: Torstrasse 25
      city: St. Gallen
      country: Switzerland
      postal-code: 9000
date: now
date-format: dddd MMM D, YYYY, HH:mm z
format:
  html:
    embed-resources: true
    theme: cosmo
# format: pdf
toc: false
number-sections: false
fig-cap-location: top
execute:
  echo: false
bibliography: ../../../documentation/references.bib
---

The identifiable victim effect is the tendency show more empathy to an identifiable individual over a group of unidentified victims who are described using numerical statistics [see, e.g., @JenniLoewenstein_1997; @SmallLoewenstein_2003; @MaierChunWongFeldman_2023]. One significant study in this domain is by @SmallLoewensteinSlovic_2006 demonstrates that donors are more likely to contribute to charitable causes when presented with specific stories about individuals, or _"identified victims"_, rather than abstract statistics about large groups. They found that personal stories evoke stronger emotional connections. 

We use DICE to create social media feeds where organic posts and advertisements compete for the participant's attention to study whether identified victims (compared to abstract statistics) cause social media advertisements to more effectively _"cut through the content clutter"_ [@Ordenes_2019] and drive ad recall for the charities posting these ads.

A key managerial question for any organization, but particularly for organizations with limited budgets and resources, like charities, is to understand how frequently they need to show their advertising to effectively cut through the clutter and generate positive outcomes from the digital ads. Particularly effective ads  will likely need less exposure to users to produce positive outcomes like brand recall while avoiding negative reactions such as ad fatigue.

A key lever in online advertising is Frequency Capping (FC), which limits how often a specific advertisement is shown to the same user within a set period. It is essential for preventing ad fatigue [see, e.g., @BraunMoe_2013; @SilbersteinShohamKlein_2023], where users become desensitized to an ad due to excessive exposure, which can lead to diminished engagement rates and a negative user experience. In addition to these indirect costs, high caps also have direct costs as the marginal effect of ad exposure can be assumed to be diminishing. By setting an optimal cap, marketers aim to balance both direct and indirect costs as well as benefits associated to an ad's visibility. 

Integrating these concepts, this study employs a 2 (identified victim vs. abstract numbers) by 4 (number of ad impressions) between-subjects design to investigate whether and how the identified victim effect drives ad recall over the course of varying degrees of ad penetration

# Experimental Design

On a high level, we create different feeds that contain both organic and sponsored posts as illustrated in @fig-design, where the first column on the left illustrates a feed that contains five sets of organic posts (grey boxes), three filler ads (yellow, green, and blue boxes) as well as one focal ad, which is either v.1 (showing an identified victim) or v.2 (showing abstract numbers). Moving from left to the right, the figure shows how the filler ads are successively replaced by the focal ad, essentially manipulating the amount of (focal-) ad impressions per feed.

![2 (identified victim vs. abstract numbers) by 4 (number of focal ad impressions) between-subjects design](../idea/treatments.png){#fig-design}

Given a frequency cap $\{1, 2, 3, 4\}$, we do neither vary the organic posts nor the filler ads between conditions. However, we do manipulate the focal ad (identified victim vs. abstract numbers). In addition, the order in which the ads are displayed is randomized between subjects too. Hence, some participants may encounter the ads in a sequence like `Ad A` > `Ad C` > `Ad B` > `Focal Ad`, whereas other see a sequence like `Focal Ad` > `Ad A` > `Ad C` > `Ad B` or any other variation. The "slots" in which an ad can be displayed remain constant.^[We expose participants to ads in 6th, 21st, 36st, and 51st position of the feed.]


# Stimuli

The two variations of the focal ad are displayed in @fig-stimuli.

::: {#fig-stimuli layout-ncol=2}

![Identified Victim Condition](../stimuli/images/creative_focal_treatment.png){#fig-ad-identified}

![Abstract Condition](../stimuli/images/creative_focal_control.png){#fig-ad-abstract}

Variations of Focal Ad
:::


```{r}
set.seed(42)
```

```{r install_packages}
#| warning: false
#| output: false

options(repos = c(CRAN = "https://cran.r-project.org")) 


if (!requireNamespace("groundhog", quietly = TRUE)) {
  install.packages("groundhog")
  library("groundhog")
}

pkgs <- c("magrittr", "data.table", "knitr", "stringr", "lubridate", 
          "ggplot2", "wesanderson", "stargazer", "Rmisc", "margins")

groundhog::groundhog.library(pkg = pkgs,
                             date = "2024-01-01")

rm(pkgs)
```

```{r layout}
layout <- theme(panel.background = element_rect(fill = "white"),
                legend.key = element_rect(fill = "white"),
                panel.grid.major.y = element_line(colour = "grey", 
                                                  linewidth = 0.25),
                axis.ticks.y = element_blank(),
                panel.grid.major.x = element_blank(),
                axis.line.x.bottom = element_line(colour = "#000000", 
                                                  linewidth = 0.5),
                axis.line.y.left = element_blank(),
                plot.title = element_text(size = rel(1))
)
```

```{r colors}
c_coral     <- "#f27981"
c_yellow    <- "#F2EA79"
c_turquoise <- "#79f2ea"
c_purple    <- "#7981f2"

c_pink   <- "#ed1e79"
c_violet <- "#662d91"

scale_color_custom_d <- function() {
  scale_color_manual(values = c(c_pink, c_violet))
}
scale_fill_custom_d <- function() {
  scale_fill_manual(values = c(c_pink, c_violet))
}
```


# Simulations

```{r set_params}
n <- 800   # sample size
a <- 4     # Coefficient for logarithmic function
b <- 0.75  # Rate parameter for logarithmic function
c <- 2     # Coefficient for linear function
intercept   <- -3   # Intercept for the logistic function
noise_level <- 0.3  # This represents 10% noise level
```

```{r}
#| eval: false
# Generate x values
cap <- runif(n, min = 0, max = 4)

# Generate condition values (binary)
condition <- rep(x = c(0, 1), times = n/2)

# Generate probabilities based on condition
p <- ifelse(test = condition == 1, 
            yes = plogis(intercept + 2 * c * cap), 
            no = plogis(intercept + c * cap))

# Adjust probabilities by adding random noise
# Assuming 'noise_level' is a parameter determining the magnitude of noise
adjusted_p <- p + rnorm(n, mean = 0, sd = noise_level * mean(p))
adjusted_p <- pmin(1, pmax(0, adjusted_p))  # Ensure probabilities stay within [0,1]


# Generate binary recall values based on probabilities
recall <- rbinom(n, size = 1, prob = adjusted_p)

# Create a data.table
float_sim <- data.table(cap = cap, 
                        recall = recall, 
                        condition = as.integer(condition))

# Fit a logistic regression model
float_model <- glm(formula = recall ~ condition + cap + condition*cap, 
                 data = float_sim, 
                 family = binomial)

# Add fitted probabilities to the data.table
float_sim[, fitted := predict(float_model, type = "response")]

ggplot(data = float_sim) +
  geom_point(mapping = aes(x = cap, 
                           y = recall, 
                           color = as.factor(condition)),
             alpha = 0.66, shape = 3) +
  geom_point(mapping = aes(x = cap, 
                           y = fitted, 
                           color = as.factor(condition)),
             alpha = 0.66) +
  labs(title = "Simulated Binary Data with Logarithmic and Linear Fits",
       x = "Frequency Caps",
       y = "Recall",
       color = "Condition") +
  scale_color_manual(values = wes_palette("BottleRocket2")) +
  layout
```

```{r integer_visualization}
#| eval: true
# Generate x values
cap <- rep(seq(from = 1, to = 4), each = n/4)

# Generate condition values (binary)
condition <- rep(x = c(0, 1), times = n/2)

# Generate probabilities based on condition
p <- ifelse(test = condition == 1, 
            yes  = plogis(intercept + c * cap), 
            no   = plogis(intercept + 1 * cap))

# Adjust probabilities by adding random noise
# Assuming 'noise_level' is a parameter determining the magnitude of noise
adjusted_p <- p + rnorm(n, mean = 0, sd = noise_level * mean(p))
adjusted_p <- pmin(1, pmax(0, adjusted_p))  # Ensure probabilities stay within [0,1]


# Generate binary recall values based on probabilities
recall <- rbinom(n, size = 1, prob = adjusted_p)

# Create a data.table
int_sim <- data.table(cap = cap, 
                        recall = recall, 
                        condition = as.integer(condition))

# Fit a logistic regression model
int_model <- glm(formula = recall ~ condition + cap + condition*cap, 
                 data = int_sim, 
                 family = binomial)

# Add fitted probabilities to the data.table
int_sim[, fitted := predict(int_model, type = "response")]

tmp <- summarySE(data = int_sim,
                 measurevar = "recall",
                 groupvars=c("cap", "condition"),
                 na.rm = FALSE,
                 conf.interval = 0.95,
                 .drop = TRUE) %>% 
  data.table()

tmp[, condition := as.factor(condition)]


ggplot(data = tmp,
       mapping = aes(x = cap, y = recall, group = condition, color = condition, lty = condition)) +
  layout +
  geom_hline(yintercept = 1) +
  scale_color_custom_d() +
  # scale_color_manual(values = wes_palette("BottleRocket2")) +
  theme(legend.position="top") +
  geom_line(show.legend=FALSE) +
  geom_errorbar(aes(ymin=recall-ci, ymax=recall+ci), width=.25, alpha = 0.5) +
  scale_y_continuous(expand = c(0, NA), limits = c(0, NA)) +
  geom_point() +
  labs(y = "Recall", x = "Number of Ad Impressions per Session", 
       color = "Condition", lty = "Condition", caption = "Bars indicate 95% confidence intervals.")
```



$$
\begin{align*}
\text{recall} &= \beta_0 + \beta_1  \text{condition} + \beta_2  \text{cap} + \beta_3 \text{condition} \times \text{cap} + \epsilon
\end{align*}
$$



```{r}
#| results: asis
lm_1 <- glm(formula = recall ~ cap + condition + cap*condition, 
            data = int_sim,
            family = binomial)

stargazer::stargazer(lm_1, 
                     dep.var.caption  = "Recall",
                     dep.var.labels   = "Logistic Regressions",
                     column.labels = c("float", "int"),
                     omit = "Constant",
                     header=FALSE,
                     type = "html",
                     digits = 2)
```


# Method and Measures

We run a 2-cell (flood-context vs. general-context) between-subjects design in which each participant faces the sponsored post.

After participants browse the social media feed, they are redirected to a Qualtrics survey that starts with basic demographic questions. Subsequently, they answer unaided and aided recall questions to indicate whether they remember seeing a _KLM_ ad. 

Finally we elicit the participants' brand attitudes towards KLM before we debrief and redirect them to Prolific.


# Primary Analysis

Our primary interest lies in the effect the context has on our three-item brand attitude measure. Hence, we compute an average of these items and compare means using a simple OLS regression (to which we subsequently add potential covariates).

We expect the flood-context to affect the brand attitude towards _KLM_ negatively, which is why we consider a one sided test.

# Secondary Analysis

We expect heterogeneous treatment effects, where recall and the (absence of) the dwell time participants allocate to the focal brand's sponsored ad moderate the treatment: Those participants who have not seen or processed the ad (approximated by a lack of dwell time and failures to recall the ad, respectively), shouldn't be affected by the brand logo.

# Population

We will recruit participants from Prolific who meet the following criteria:

- Approval Rate >= 99%
- First Language == ‘English’
- Location == ‘USA’

# Sample Size 

As stated above, we expect our treatment effect to be heterogeneous. As a consequence, the sponsored ad can only affect brand evaluations if it cuts through the content clutter [@Ordenes_2019].

Hence, we recruit 300 participants in a first pilot. To this end, we create a database containing 500 rows.

# Exclusion Criteria

We will only consider complete observations, that is, data from participants who browsed through the feed, answered the qualtrics survey and who were redirected to Prolific with a functional completion code.

Because we gather process data, such as dwell time, we have tools to assess the data quality [@CuskleySulik_2024] -- at least during the exposure to the social media feed. If these data reveal inattentive participants, for instance, we may exclude them too but label the resulting analyses as exploratory.

# Prior Data Collection

We did not collect any data before.

# References

<!--

## Legacy

```{r}
#| eval: false
# Generate x values
cap <- runif(n, min = 0, max = 10)

# Generate z values (binary)
condition <- rbinom(n, size = 1, prob = 0.5)

# Generate y values based on z
recall <- ifelse(test = condition == 1, 
                 yes = a * (1 - exp(-b * cap)), 
                 no = c * cap + rnorm(n, mean = 0, sd = 0.5))

# Create a data.table
dt <- data.table(cap = cap, 
                 recall = recall, 
                 condition = condition)
```


```{r}
#| eval: false
# Fit a linear regression model with interaction terms
model <- lm(formula = recall ~ condition * (cap + I((1 - exp(-b * cap)))), 
            data = dt)


# Plot the data and the fitted curves
plot(dt$cap, dt$recall, 
     main = "Simulated Data with Logarithmic and Linear Fits", 
     xlab = "Frequency Caps", 
     ylab = "Recall", 
     col = ifelse(dt$z == 1, "blue", "green"), 
     pch = 19)
lines(sort(dt$cap[dt$condition == 1]), predict(model, newdata = dt[dt$condition == 1][order(cap)]), col = "red", lwd = 2)
lines(sort(dt$cap[dt$condition == 0]), predict(model, newdata = dt[dt$condition == 0][order(cap)]), col = "orange", lwd = 2)

```

```{r float_simulation}
#| eval: false
# Generate cap values
cap <- runif(n, min = 0, max = 10)

# Generate probabilities based on condition
p <- ifelse(test = condition == 1, 
            yes = plogis(intercept + a * (1 - exp(-b * cap))), 
            no = plogis(intercept + c * cap))

# Generate binary recall values based on probabilities
recall <- rbinom(n, size = 1, prob = p)

# Create a data.table
float_sim <- data.table(cap = cap, 
                        recall = recall, 
                        condition = as.integer(condition))

# Fit a logistic regression model
float_model <- glm(formula = recall ~ condition * (cap + I((1 - exp(-b * cap)))),
                   data = float_sim,
                   family = binomial)


# Add fitted probabilities to the data.table
float_sim[, fitted := predict(float_model, type = "response")]
```


```{r integer_simulation}
#| eval: false
# Generate cap values (integer values from 1 to 5)
cap <- rep(seq(from = 1, to = 5), each = n/5)

# Generate probabilities based on condition
p <- ifelse(test = condition == 1, 
            yes = plogis(intercept + a * (1 - exp(-b * cap))), 
            no = plogis(intercept + c * cap))

# Generate binary recall values based on probabilities
recall <- rbinom(n, size = 1, prob = p)

# Create a data.table
int_sim <- data.table(cap = cap, 
                      recall = recall, 
                      condition = as.integer(condition))

# Fit a logistic regression model
int_model <- glm(formula = recall ~ condition * (cap + I((1 - exp(-b * cap)))), 
                 data = int_sim, 
                 family = binomial)

# Add fitted probabilities to the data.table
int_sim[, fitted := predict(int_model, type = "response")]
```


```{r float_viz}
#| eval: false
ggplot(data = float_sim) +
  geom_point(mapping = aes(x = cap, 
                           y = recall, 
                           color = as.factor(condition)),
             alpha = 0.66, shape = 3) +
  geom_point(mapping = aes(x = cap, 
                           y = fitted, 
                           color = as.factor(condition)),
             alpha = 0.66) +
  labs(title = "Simulated Binary Data with Logarithmic and Linear Fits",
       x = "Frequency Caps",
       y = "Recall",
       color = "Condition") +
  layout
```

```{r integer_viz_1}
#| eval: false
ggplot(data = int_sim) +
  geom_jitter(mapping = aes(x = cap, 
                            y = recall, 
                            color = as.factor(condition)),
              height = 0.05, width = 0.1,
              alpha = 0.66, shape = 3) +
  geom_jitter(mapping = aes(x = cap, 
                            y = fitted, 
                            color = as.factor(condition)),
              height = 0.05, width = 0.1,
              alpha = 0.5) +
  labs(title = "Simulated Binary Data with Logarithmic and Linear Fits",
       x = "Frequency Caps",
       y = "Recall",
       color = "Condition") +
  layout
```

```{r integer_viz_2}
#| eval: false
tmp <- summarySE(data = int_sim,
                 measurevar = "recall",
                 groupvars=c("cap", "condition"),
                 na.rm = FALSE,
                 conf.interval = 0.95,
                 .drop = TRUE) %>% 
  data.table()

tmp[, condition := as.factor(condition)]


ggplot(data = tmp,
       mapping = aes(x = cap, y = recall, group = condition, color = condition)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE, lty=2) +
  geom_errorbar(aes(ymin=recall-ci, ymax=recall+ci), width=.25, alpha = 0.5) +
  geom_point() +
  labs(y = "Recall", x = "Frequency Caps", 
       color = "Condition", caption = "Bars indicate 95% confidence intervals.")
```

```{r}
#| eval: false
#| results: asis

float_lm <- glm(formula = recall ~ cap + condition + cap*condition, 
            data = float_sim,
            family = binomial)
int_lm <- glm(formula = recall ~ cap + condition + cap*condition, 
            data = int_sim,
            family = binomial)


stargazer::stargazer(float_model, int_model, float_lm, int_lm, 
                     dep.var.caption  = "Recall",
                     dep.var.labels   = "Logistic Regressions",
                     column.labels = c("float", "int", "float", "int"),
                     header=FALSE,
                     type = "html",
                     digits = 2)
```

-->


# References